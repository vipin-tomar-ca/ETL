# =============================================
# Spark Default Configuration for Multi-Database Transform
# =============================================
# This file contains optimized Spark configurations for large-scale ETL processing
# Optimized for handling 1 TB/day with 10-20 Spark nodes

# Application Configuration
spark.app.name                    MultiDBTransform
spark.master                      spark://spark-master:7077
spark.driver.memory               4g
spark.driver.cores                2
spark.executor.memory             8g
spark.executor.cores              4
spark.executor.memoryOverhead     2g
spark.driver.memoryOverhead       1g

# Parallelism and Partitioning
spark.default.parallelism         200
spark.sql.shuffle.partitions      200
spark.sql.adaptive.enabled        true
spark.sql.adaptive.coalescePartitions.enabled true
spark.sql.adaptive.skewJoin.enabled true
spark.sql.adaptive.localShuffleReader.enabled true
spark.sql.adaptive.advisoryPartitionSizeInBytes 134217728

# File and I/O Configuration
spark.sql.files.maxPartitionBytes 134217728
spark.sql.files.openCostInBytes   4194304
spark.sql.files.ignoreCorruptFiles true
spark.sql.files.ignoreMissingFiles true

# Broadcast and Join Configuration
spark.sql.broadcastTimeout        3600
spark.sql.autoBroadcastJoinThreshold 100485760
spark.sql.adaptive.autoBroadcastJoinThreshold 100485760

# Arrow Configuration (for better performance with .NET)
spark.sql.execution.arrow.pyspark.enabled true
spark.sql.execution.arrow.pyspark.fallback.enabled true
spark.sql.execution.arrow.maxRecordsPerBatch 10000

# Serialization
spark.serializer                 org.apache.spark.serializer.KryoSerializer
spark.kryoserializer.buffer.max  1024m
spark.kryo.registrationRequired  false

# Memory Management
spark.memory.fraction             0.8
spark.memory.storageFraction     0.3
spark.memory.offHeap.enabled     true
spark.memory.offHeap.size        2g

# Garbage Collection
spark.executor.extraJavaOptions  -XX:+UseG1GC -XX:MaxGCPauseMillis=200 -XX:+UnlockExperimentalVMOptions -XX:+UseStringDeduplication -XX:+UseCompressedOops -XX:+UseCompressedClassPointers
spark.driver.extraJavaOptions    -XX:+UseG1GC -XX:MaxGCPauseMillis=200 -XX:+UnlockExperimentalVMOptions -XX:+UseStringDeduplication -XX:+UseCompressedOops -XX:+UseCompressedClassPointers

# Network Configuration
spark.network.timeout            800s
spark.executor.heartbeatInterval 60s
spark.rpc.askTimeout             600s
spark.rpc.lookupTimeout          600s
spark.rpc.io.threads             64
spark.rpc.netty.dispatcher.numThreads 64

# Dynamic Allocation
spark.dynamicAllocation.enabled  true
spark.dynamicAllocation.minExecutors 5
spark.dynamicAllocation.maxExecutors 20
spark.dynamicAllocation.initialExecutors 10
spark.dynamicAllocation.executorIdleTimeout 60s
spark.dynamicAllocation.schedulerBacklogTimeout 1s
spark.dynamicAllocation.sustainedSchedulerBacklogTimeout 1s

# Event Logging and Monitoring
spark.eventLog.enabled           true
spark.eventLog.dir               hdfs:///spark-events
spark.eventLog.compress          true
spark.eventLog.compression.codec snappy
spark.sql.statistics.size.autoUpdate.enabled true
spark.sql.statistics.histogram.enabled true

# JDBC Configuration
spark.sql.jdbc.batchsize         10000
spark.sql.jdbc.fetchsize         10000
spark.sql.jdbc.sessionInitStatement "SET QUOTED_IDENTIFIER ON"

# Security (disabled for development)
spark.authenticate               false
spark.authenticate.enableSaslEncryption false
spark.network.crypto.enabled     false
spark.ssl.enabled                false

# Logging
spark.sql.execution.arrow.pyspark.selfDestruct.enabled true
spark.sql.execution.arrow.pyspark.selfDestruct.maxRecordBatch 10000

# Performance Tuning
spark.sql.adaptive.skewJoin.skewedPartitionThresholdInBytes 256MB
spark.sql.adaptive.skewJoin.skewedPartitionFactor 5
spark.sql.adaptive.advisoryPartitionSizeInBytes 128MB
spark.sql.adaptive.coalescePartitions.minPartitionNum 1
spark.sql.adaptive.coalescePartitions.initialPartitionNum 200

# Speculation (for handling slow tasks)
spark.speculation                true
spark.speculation.multiplier     1.5
spark.speculation.quantile       0.75

# External Shuffle Service
spark.shuffle.service.enabled    true
spark.shuffle.service.port       7337

# History Server
spark.history.fs.logDirectory    hdfs:///spark-events
spark.history.fs.update.interval 10s
spark.history.fs.cleaner.enabled true
spark.history.fs.cleaner.interval 1d
spark.history.fs.cleaner.maxAge  7d

# UI Configuration
spark.ui.port                    4040
spark.ui.retainedJobs            100
spark.ui.retainedStages          100
spark.ui.retainedTasks           1000

# Streaming Configuration (if needed)
spark.sql.streaming.checkpointLocation hdfs:///checkpoints
spark.sql.streaming.minBatchesToRetain 2

# Hive Configuration (if using Hive)
spark.sql.warehouse.dir          hdfs:///user/hive/warehouse
spark.hadoop.hive.metastore.uris thrift://hive-metastore:9083

# Delta Lake Configuration (if using Delta)
spark.sql.extensions             io.delta.sql.DeltaSparkSessionExtension
spark.sql.catalog.spark_catalog  org.apache.spark.sql.delta.catalog.DeltaCatalog

# Custom Properties for Multi-Database ETL
spark.sql.adaptive.optimizeSkewedJoin.enabled true
spark.sql.adaptive.optimizeSkewedJoin.skewedPartitionThresholdInBytes 256MB
spark.sql.adaptive.optimizeSkewedJoin.skewedPartitionFactor 5
spark.sql.adaptive.optimizeSkewedJoin.skewedPartitionMaxSplits 5

# JDBC Connection Pooling
spark.sql.jdbc.connectionPool.enabled true
spark.sql.jdbc.connectionPool.maxConnections 20
spark.sql.jdbc.connectionPool.initialConnections 5
spark.sql.jdbc.connectionPool.timeout 300s
spark.sql.jdbc.connectionPool.validationTimeout 5s

# Performance Monitoring
spark.sql.statistics.colStats.enabled true
spark.sql.statistics.histogram.enabled true
spark.sql.statistics.size.autoUpdate.enabled true
spark.sql.statistics.size.autoUpdate.enabled true
